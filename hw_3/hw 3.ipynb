{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostClassifier\n",
    "from collections import defaultdict\n",
    "from pymystem3 import Mystem\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '«'},\n",
       " {'analysis': [{'gr': 'S,ед,жен,неод=(вин|им)', 'lex': 'роснефть', 'wt': 1}],\n",
       "  'text': 'Роснефть'},\n",
       " {'text': '», «'},\n",
       " {'analysis': [{'gr': 'S,жен,неод=(пр,ед|вин,мн|дат,ед|род,ед|им,мн)',\n",
       "    'lex': 'ведомость',\n",
       "    'wt': 1}],\n",
       "  'text': 'Ведомости'},\n",
       " {'text': '», '},\n",
       " {'analysis': [{'gr': 'S,муж,неод=(пр,мн|пр,ед|вин,мн|вин,ед|дат,мн|дат,ед|род,мн|род,ед|твор,мн|твор,ед|им,мн|им,ед)',\n",
       "    'lex': 'ыва',\n",
       "    'qual': 'bastard',\n",
       "    'wt': 0.2000164368}],\n",
       "  'text': 'ыва'},\n",
       " {'text': '\\n'}]"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Mystem()\n",
    "m.analyze('«Роснефть», «Ведомости», ыва')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrain():\n",
    "    (_, _, filenames) = next(walk('Collection3'))\n",
    "    lines = []\n",
    "    for name in filenames:\n",
    "        if 'ann' in name:\n",
    "            with open('Collection3/' + name) as f:\n",
    "                lines += [line for line in f.readlines()]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readRaw():\n",
    "    (_, _, filenames) = next(walk('Collection3'))\n",
    "    lines = []\n",
    "    for name in filenames:\n",
    "        if 'ann' not in name:\n",
    "            with open('Collection3/' + name) as f:\n",
    "                lines += [line for line in f.readlines()]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "w2v_fpath = \"all.norm-sz100-w10-cb0-it1-min100.w2v\"\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(w2v_fpath, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toW2vTag(tag):\n",
    "    if tag == 'S':\n",
    "        return 'NOUN'\n",
    "    if tag == 'A':\n",
    "        return 'ADJ'\n",
    "    return tag\n",
    "\n",
    "def analyzeWord(word, isUsed):\n",
    "    features = []\n",
    "    wordClean = re.sub(r'[^а-яА-Я]', r'', word)\n",
    "    if len(wordClean) < 3:\n",
    "        raise Exception()\n",
    "    raw = m.analyze(wordClean)[0]['analysis'][0]\n",
    "    tag = raw['gr'][0]\n",
    "    isNoun = tag == 'S'\n",
    "    features.append(isNoun)\n",
    "    vowels = set('аоиеёэыуюя')\n",
    "    isName = 'имя' in ['gr'] or 'фам' in raw['gr']\n",
    "    features.append(isName)\n",
    "    isCapital = np.mean([char.isupper() for char in word])\n",
    "    vowelsCnt = np.mean([letter in vowels for letter in wordClean.lower()])\n",
    "    features.append(isCapital)\n",
    "    isInBrackets = word == wordClean\n",
    "    features.append(isInBrackets)\n",
    "    features.append(vowelsCnt)\n",
    "    w2v_name = 0\n",
    "    w2v_surname = 0\n",
    "    w2v_org = 0\n",
    "    w2v_person = 0\n",
    "    w2v_pers = 0\n",
    "    w2v_company = 0\n",
    "    w2v_ved = 0\n",
    "    w2v_corp = 0\n",
    "    w2v_tag = toW2vTag(tag)\n",
    "    try:\n",
    "        w2v_word =  raw['lex'].lower()\n",
    "    except:\n",
    "        w2v_word = word.lower()\n",
    "    try: w2v_pers = w2v.similarity('персона', w2v_word)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    try: w2v_surname = w2v.similarity('политик', w2v_word)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    try: w2v_name = w2v.similarity('имя', w2v_word)\n",
    "    except: pass\n",
    "    try: w2v_org = w2v.similarity('организация', w2v_word)\n",
    "    except: pass\n",
    "    try: w2v_person = w2v.similarity('человек', w2v_word)\n",
    "    except: pass\n",
    "    try: w2v_company = w2v.similarity('компания', w2v_word)\n",
    "    except: pass\n",
    "    try: w2v_ved = w2v.similarity('ведомство', w2v_word)\n",
    "    except: pass\n",
    "    try: w2v_corp = w2v.similarity('новости', w2v_word)\n",
    "    except: pass\n",
    "    features.append(w2v_org)\n",
    "    features.append(w2v_pers)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def analyzeSentence(sentence, isUsed):\n",
    "    features = []\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            if np.random.random() < 0.3:\n",
    "                features.append(analyzeWord(word, isUsed))\n",
    "        except:\n",
    "            continue\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = []\n",
    "train_label = []\n",
    "isUsed = defaultdict(bool)\n",
    "m = Mystem()\n",
    "\n",
    "for line in readTrain():\n",
    "    label = line.split()[1]\n",
    "    for word in line.split()[4:]:\n",
    "        try: \n",
    "            train_data.append(analyzeWord(word, isUsed))\n",
    "            train_label.append(label)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "for line in readRaw():\n",
    "    label = 'LOC'\n",
    "    features = analyzeSentence(line, isUsed)\n",
    "    train_data += features\n",
    "    train_label += [label for word in features]\n",
    "\n",
    "cat_features = [0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_label, test_size=0.2, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.8920744\tbest: 0.8920744 (0)\ttotal: 24.7ms\tremaining: 1m 14s\n",
      "100:\ttest: 0.9300495\tbest: 0.9300495 (100)\ttotal: 1.27s\tremaining: 36.6s\n",
      "200:\ttest: 0.9385001\tbest: 0.9385001 (200)\ttotal: 2.49s\tremaining: 34.7s\n",
      "300:\ttest: 0.9424559\tbest: 0.9424559 (300)\ttotal: 3.73s\tremaining: 33.5s\n",
      "400:\ttest: 0.9453933\tbest: 0.9453933 (400)\ttotal: 4.96s\tremaining: 32.2s\n",
      "500:\ttest: 0.9477265\tbest: 0.9477265 (500)\ttotal: 6.21s\tremaining: 31s\n",
      "600:\ttest: 0.9492319\tbest: 0.9492319 (600)\ttotal: 7.44s\tremaining: 29.7s\n",
      "700:\ttest: 0.9506031\tbest: 0.9506031 (700)\ttotal: 8.67s\tremaining: 28.4s\n",
      "800:\ttest: 0.9516157\tbest: 0.9516157 (800)\ttotal: 9.92s\tremaining: 27.2s\n",
      "900:\ttest: 0.9525345\tbest: 0.9525345 (900)\ttotal: 11.2s\tremaining: 26s\n",
      "1000:\ttest: 0.9532962\tbest: 0.9532962 (1000)\ttotal: 12.4s\tremaining: 24.8s\n",
      "1100:\ttest: 0.9539238\tbest: 0.9539238 (1100)\ttotal: 13.7s\tremaining: 23.6s\n",
      "1200:\ttest: 0.9545369\tbest: 0.9545369 (1200)\ttotal: 14.9s\tremaining: 22.3s\n",
      "1300:\ttest: 0.9550359\tbest: 0.9550359 (1300)\ttotal: 16.1s\tremaining: 21.1s\n",
      "1400:\ttest: 0.9554912\tbest: 0.9554912 (1400)\ttotal: 17.4s\tremaining: 19.9s\n",
      "1500:\ttest: 0.9557554\tbest: 0.9557554 (1500)\ttotal: 18.7s\tremaining: 18.6s\n",
      "1600:\ttest: 0.9561504\tbest: 0.9561504 (1600)\ttotal: 19.9s\tremaining: 17.4s\n",
      "1700:\ttest: 0.9564818\tbest: 0.9564818 (1700)\ttotal: 21.1s\tremaining: 16.2s\n",
      "1800:\ttest: 0.9567576\tbest: 0.9567576 (1800)\ttotal: 22.8s\tremaining: 15.2s\n",
      "1900:\ttest: 0.9570074\tbest: 0.9570074 (1900)\ttotal: 24.3s\tremaining: 14.1s\n",
      "2000:\ttest: 0.9571786\tbest: 0.9571786 (2000)\ttotal: 25.6s\tremaining: 12.8s\n",
      "2100:\ttest: 0.9574323\tbest: 0.9574323 (2100)\ttotal: 26.8s\tremaining: 11.5s\n",
      "2200:\ttest: 0.9576323\tbest: 0.9576323 (2200)\ttotal: 28.1s\tremaining: 10.2s\n",
      "2300:\ttest: 0.9577999\tbest: 0.9577999 (2300)\ttotal: 29.3s\tremaining: 8.91s\n",
      "2400:\ttest: 0.9580182\tbest: 0.9580182 (2400)\ttotal: 30.6s\tremaining: 7.63s\n",
      "2500:\ttest: 0.9581545\tbest: 0.9581545 (2500)\ttotal: 31.8s\tremaining: 6.35s\n",
      "2600:\ttest: 0.9583154\tbest: 0.9583154 (2600)\ttotal: 33.1s\tremaining: 5.07s\n",
      "2700:\ttest: 0.9584475\tbest: 0.9584475 (2700)\ttotal: 34.3s\tremaining: 3.8s\n",
      "2800:\ttest: 0.9585398\tbest: 0.9585398 (2800)\ttotal: 35.6s\tremaining: 2.53s\n",
      "2900:\ttest: 0.9586537\tbest: 0.9586537 (2900)\ttotal: 36.9s\tremaining: 1.26s\n",
      "2999:\ttest: 0.9587478\tbest: 0.9587478 (2999)\ttotal: 38.1s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9587477888\n",
      "bestIteration = 2999\n",
      "\n",
      "Count of trees in model = 3000\n"
     ]
    }
   ],
   "source": [
    "cat_features = [0, 1, 3]\n",
    "\n",
    "train_dataset = Pool(data=X_train,\n",
    "                     label=y_train,\n",
    "                     cat_features=cat_features)\n",
    "    \n",
    "test_dataset = Pool(data=X_test,\n",
    "                     label=y_test,\n",
    "                     cat_features=cat_features)\n",
    "# Initialize CatBoostClassifier\n",
    "model = CatBoostClassifier(iterations=3000,\n",
    "                           depth=7,\n",
    "                           l2_leaf_reg= 10\n",
    "                           , metric_period=100,\n",
    "                           eval_metric='AUC',\n",
    "                           loss_function='MultiClass')\n",
    "# Fit model with `use_best_model=True`\n",
    "\n",
    "model.fit(train_dataset,\n",
    "          eval_set=test_dataset,\n",
    "          use_best_model=True)\n",
    "\n",
    "print(\"Count of trees in model = {}\".format(model.tree_count_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.0572428 ,  4.44282262, 16.86103447,  6.09809877,  7.98568046,\n",
       "        7.69773675,  6.78972031,  8.64950592,  8.19359677,  7.09281682,\n",
       "        6.20103142,  7.26873953,  7.66197336])"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findOccurrences(s, ch):\n",
    "    return [i for i, letter in enumerate(s) if letter == ch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processLine(line):\n",
    "    words = line.split()\n",
    "    features = []\n",
    "    ans = \"\"\n",
    "    if len(line) < 2:\n",
    "        print('OK')\n",
    "        return\n",
    "    for word in words:\n",
    "        try:\n",
    "            if 'РБК' in word:\n",
    "                pos = line.find(word)\n",
    "                ans += str(pos) + ' ' + str(len(word)) + ' ORG '\n",
    "                continue\n",
    "            if re.search('[a-zA-Z]', word) is not None:\n",
    "                pos = line.find(word)\n",
    "                ans += str(pos) + ' ' + str(len(word)) + ' ORG '\n",
    "                continue\n",
    "            word = word.replace(',', '').replace('.', '').replace('?', '').replace('!', '').replace('\\n', '')\n",
    "            pred_class = model.predict(analyzeWord(word, isUsed))\n",
    "            if pred_class[0] == 'ORG':\n",
    "                pos = line.find(word)\n",
    "                ans += str(pos) + ' ' + str(len(word)) + ' ORG '\n",
    "            if pred_class[0] == 'PER':\n",
    "                pos = line.find(word)\n",
    "                ans += str(pos) + ' ' + str(len(word)) + ' PERSON  '  \n",
    "        except Exception as e:\n",
    "            #print(word)\n",
    "            pass\n",
    "    with open('out.txt', 'a') as f:\n",
    "        f.write(ans + 'EOL\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTest():\n",
    "    with open('dataset_40163_1.txt') as f:\n",
    "        [processLine(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "m = Mystem()\n",
    "try:\n",
    "    os.remove('out.txt')\n",
    "except: pass\n",
    "processTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
